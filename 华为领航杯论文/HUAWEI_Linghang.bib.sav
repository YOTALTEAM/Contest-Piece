@book{Lee2012,
  author    = {John M. Lee},
  publisher = {Springer New York},
  title     = {Introduction to Smooth Manifolds},
  year      = {2012},
  doi       = {10.1007/978-1-4419-9982-5}
}

@article{Winograd1968,
  author     = {Winograd, S.},
  journal    = {IEEE Trans. Comput.},
  title      = {A New Algorithm for Inner Product},
  year       = {1968},
  issn       = {0018-9340},
  month      = {6},
  number     = {7},
  pages      = {693-694},
  volume     = {17},
  abstract   = {Abstract In this note we describe a new way of computing the inner product of two vectors. This method cuts down the number of multiplications required when we want to perform a large number of inner products on a smaller set of vectors. In particular, we obtain that the product of two n n matrices can be performed using roughly n3/2 multiplications instead of the n3multiplications which the regular method necessitates.},
  address    = {USA},
  doi        = {10.1109/TC.1968.227420},
  issue_date = {July 1968},
  keywords   = {Index terms Algorithm, Index terms—Algorithm, matrix multiplication, inner product, solution of linear equations., matrix inversion},
  numpages   = {2},
  publisher  = {IEEE Computer Society},
  url        = {https://doi.org/10.1109/TC.1968.227420}
}

@article{Gholami2021,
  author        = {Amir Gholami and Sehoon Kim and Zhen Dong and Zhewei Yao and Michael W. Mahoney and Kurt Keutzer},
  title         = {A Survey of Quantization Methods for Efficient Neural Network Inference},
  year          = {2021},
  month         = mar,
  abstract      = {As soon as abstract mathematical computations were adapted to computation on digital computers, the problem of efficient representation, manipulation, and communication of the numerical values in those computations arose. Strongly related to the problem of numerical representation is the problem of quantization: in what manner should a set of continuous real-valued numbers be distributed over a fixed discrete set of numbers to minimize the number of bits required and also to maximize the accuracy of the attendant computations? This perennial problem of quantization is particularly relevant whenever memory and/or computational resources are severely restricted, and it has come to the forefront in recent years due to the remarkable performance of Neural Network models in computer vision, natural language processing, and related areas. Moving from floating-point representations to low-precision fixed integer values represented in four bits or less holds the potential to reduce the memory footprint and latency by a factor of 16x; and, in fact, reductions of 4x to 8x are often realized in practice in these applications. Thus, it is not surprising that quantization has emerged recently as an important and very active sub-area of research in the efficient implementation of computations associated with Neural Networks. In this article, we survey approaches to the problem of quantizing the numerical values in deep Neural Network computations, covering the advantages/disadvantages of current methods. With this survey and its organization, we hope to have presented a useful snapshot of the current research in quantization for Neural Networks and to have given an intelligent organization to ease the evaluation of future research in this area.},
  archiveprefix = {arXiv},
  eprint        = {2103.13630},
  file          = {:http\://arxiv.org/pdf/2103.13630v3:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV}
}

@article{Jiang2020,
  author  = {Jiang, Honglan and Santiago, Francisco Javier Hernandez and Mo, Hai and Liu, Leibo and Han, Jie},
  journal = {Proceedings of the IEEE},
  title   = {Approximate Arithmetic Circuits: A Survey, Characterization, and Recent Applications},
  year    = {2020},
  number  = {12},
  pages   = {2108-2135},
  volume  = {108},
  doi     = {10.1109/JPROC.2020.3006451}
}

@Book{Cormen2022,
  author    = {Cormen, Thomas H and Leiserson, Charles E and Rivest, Ronald L and Stein, Clifford},
  publisher = {MIT press},
  title     = {Introduction to algorithms},
  year      = {2022},
  edition   = {4},
}

@Book{Li2010,
  author    = {Li, Chao and Ruan, Wei and Zhang, Long and Zhang, Xiang},
  publisher = {TsingHua University Press},
  title     = {Mathematical Principle of Computer Algebra System},
  year      = {2010},
  isbn      = {9787302230106},
  groups    = {maTHμ Project},
  url       = {https://mathmu.github.io/MTCAS/mtcas.pdf},
}

@Book{Nussbaumer1982,
  author    = {Henri J. Nussbaumer},
  publisher = {Springer Berlin Heidelberg},
  title     = {Fast Fourier Transform and Convolution Algorithms},
  year      = {1982},
  doi       = {10.1007/978-3-642-81897-4},
}

@Article{Karatsuba1995,
  author    = {Karatsuba, Anatolii Alexeevich},
  journal   = {Proceedings of the Steklov Institute of Mathematics-Interperiodica Translation},
  title     = {The complexity of computations},
  year      = {1995},
  pages     = {169--183},
  volume    = {211},
  publisher = {Providence, RI: American Mathematical Society},
}

@Article{Creswell2018,
  author        = {Antonia Creswell and Tom White and Vincent Dumoulin and Kai Arulkumaran and Biswa Sengupta and Anil A Bharath},
  journal       = {{IEEE} Signal Processing Magazine},
  title         = {Generative Adversarial Networks: An Overview},
  year          = {2018},
  month         = {jan},
  number        = {1},
  pages         = {53--65},
  volume        = {35},
  abstract      = {Generative adversarial networks (GANs) provide a way to learn deep representations without extensively annotated training data. They achieve this through deriving backpropagation signals through a competitive process involving a pair of networks. The representations that can be learned by GANs may be used in a variety of applications, including image synthesis, semantic image editing, style transfer, image super-resolution and classification. The aim of this review paper is to provide an overview of GANs for the signal processing community, drawing on familiar analogies and concepts where possible. In addition to identifying different methods for training and constructing GANs, we also point to remaining challenges in their theory and application.},
  archiveprefix = {arXiv},
  date          = {2017-10-19},
  doi           = {10.1109/msp.2017.2765202},
  eprint        = {1710.07035},
  file          = {:http\://arxiv.org/pdf/1710.07035v1:PDF},
  keywords      = {cs.CV},
  primaryclass  = {cs.CV},
  publisher     = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Courbariaux2014,
  author        = {Matthieu Courbariaux and Yoshua Bengio and Jean-Pierre David},
  title         = {Training deep neural networks with low precision multiplications},
  year          = {2014},
  month         = dec,
  abstract      = {Multipliers are the most space and power-hungry arithmetic operators of the digital implementation of deep neural networks. We train a set of state-of-the-art neural networks (Maxout networks) on three benchmark datasets: MNIST, CIFAR-10 and SVHN. They are trained with three distinct formats: floating point, fixed point and dynamic fixed point. For each of those datasets and for each of those formats, we assess the impact of the precision of the multiplications on the final error after training. We find that very low precision is sufficient not just for running trained networks but also for training them. For example, it is possible to train Maxout networks with 10 bits multiplications.},
  archiveprefix = {arXiv},
  eprint        = {1412.7024},
  file          = {:http\://arxiv.org/pdf/1412.7024v5:PDF},
  keywords      = {cs.LG, cs.CV, cs.NE},
  primaryclass  = {cs.LG},
}

@Article{Goodfellow2014,
  author        = {Ian J. Goodfellow and Jean Pouget-Abadie and Mehdi Mirza and Bing Xu and David Warde-Farley and Sherjil Ozair and Aaron Courville and Yoshua Bengio},
  title         = {Generative Adversarial Networks},
  year          = {2014},
  month         = jun,
  abstract      = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  archiveprefix = {arXiv},
  eprint        = {1406.2661},
  file          = {:http\://arxiv.org/pdf/1406.2661v1:PDF},
  keywords      = {stat.ML, cs.LG},
  primaryclass  = {stat.ML},
}

@Article{Booth1951,
  author    = {Booth, Andrew D},
  journal   = {The Quarterly Journal of Mechanics and Applied Mathematics},
  title     = {A signed binary multiplication technique},
  year      = {1951},
  number    = {2},
  pages     = {236--240},
  volume    = {4},
  publisher = {Oxford University Press},
}

@Book{Parhi2007,
  author    = {Parhi, Keshab K},
  publisher = {John Wiley \& Sons},
  title     = {VLSI digital signal processing systems: design and implementation},
  year      = {2007},
}

@Article{Liu2019,
  author    = {Liu, Weiqiang and Liao, Qicong and Qiao, Fei and Xia, Weijie and Wang, Chenghua and Lombardi, Fabrizio},
  journal   = {IEEE Transactions on Circuits and Systems I: Regular Papers},
  title     = {Approximate designs for fast Fourier transform (FFT) with application to speech recognition},
  year      = {2019},
  number    = {12},
  pages     = {4727--4739},
  volume    = {66},
  publisher = {IEEE},
}

% Data-Sheet Artix-7
@Misc{2020,
  author       = {Null},
  howpublished = {\url{https://www.xilinx.com/content/dam/xilinx/support/documents/data_sheets/ds180_7Series_Overview.pdf}},
  title        = {7 Series {FPGA}s Data Sheet: Overview},
  year         = {2020},
}


@Article{BishehNiasar2021,
  author    = {Bisheh Niasar, Mojtaba and Azarderakhsh, Reza and Mozaffari Kermani, Mehran},
  journal   = {IEEE Transactions on Circuits and Systems I: Regular Papers},
  title     = {Instruction-Set Accelerated Implementation of {CRYSTALS}-{K}yber},
  year      = {2021},
  number    = {11},
  pages     = {4648--4659},
  volume    = {68},
  publisher = {IEEE},
}

% zhaohui
@Article{Chen2021,
  author    = {Chen, Zhaohui and Ma, Yuan and Chen, Tianyu and Lin, Jingqiang and Jing, Jiwu},
  journal   = {Integration},
  title     = {High-performance area-efficient polynomial ring processor for {CRYSTALS}-{K}yber on {FPGA}s},
  year      = {2021},
  pages     = {25--35},
  volume    = {78},
  publisher = {Elsevier},
}

@Article{Guo2021,
  author    = {Guo, Wenbo and Li, Shuguo and Kong, Liang},
  journal   = {IEEE Transactions on Circuits and Systems II: Express Briefs},
  title     = {An Efficient Implementation of {K}yber},
  year      = {2021},
  publisher = {IEEE},
}


% liejun
@InProceedings{Ma2021,
  author       = {Ma, Liejun and Wu, Xingjun and Bai, Guoqiang},
  booktitle    = {2021 International Conference on Communications, Information System and Computer Engineering (CISCE)},
  title        = {Parallel polynomial multiplication optimized scheme for {CRYSTALS}-{K}yber Post-Quantum Cryptosystem based on {FPGA}},
  year         = {2021},
  organization = {IEEE},
  pages        = {361--365},
}


@Article{Xing2019,
  author    = {Xing, Yufei and Li, Shuguo},
  journal   = {IEEE Transactions on Circuits and Systems I: Regular Papers},
  title     = {An efficient implementation of the {N}ew{H}ope key exchange on {FPGA}s},
  year      = {2019},
  number    = {3},
  pages     = {866--878},
  volume    = {67},
  publisher = {IEEE},
}

% ferhat
@InProceedings{Yarman2021,
  author       = {Yarman, Ferhat and Mert, Ahmet Can and {\"O}zt{\"u}rk, Erdin{\c{c}} and Sava{\c{s}}, Erkay},
  booktitle    = {2021 Design, Automation \& Test in Europe Conference \& Exhibition (DATE)},
  title        = {A hardware accelerator for polynomial multiplication operation of {CRYSTALS}-{K}yber {PQC} scheme},
  year         = {2021},
  organization = {IEEE},
  pages        = {1020--1025},
}

@Comment{jabref-meta: databaseType:bibtex;}
